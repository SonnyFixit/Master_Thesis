import os
import numpy as np
from FM_QualityClassification import create_classification_file

# Paths to data and classification files
folder_path = r'C:\GitRepositories\Master_Thesis\Data\segmented_imu_data_frames'
binary_classification_file = r'C:\GitRepositories\Master_Thesis\FM_QualityClassification_Binary.txt'
three_class_classification_file = r'C:\GitRepositories\Master_Thesis\FM_QualityClassification_ThreeClass.txt'

imu_suffixes = ['DataFramesLF.npy', 'DataFramesLH.npy', 'DataFramesRF.npy', 'DataFramesRH.npy']

# Check if classification files exist and communicate their status
def check_classification_files():
    if os.path.exists(binary_classification_file):
        print(f"Binary classification file '{binary_classification_file}' found.")
    else:
        print(f"Binary classification file '{binary_classification_file}' not found. Creating it.")
        create_classification_file(r'D:\Master_Thesis\MasterThesis_Documents_Review\Data acquisition log file.xlsx', binary=True)

    if os.path.exists(three_class_classification_file):
        print(f"Three-class classification file '{three_class_classification_file}' found.")
    else:
        print(f"Three-class classification file '{three_class_classification_file}' not found. Creating it.")
        create_classification_file(r'D:\Master_Thesis\MasterThesis_Documents_Review\Data acquisition log file.xlsx', binary=False)

# Function to load classification data
def load_classifications(file_path):
    classifications = {}
    with open(file_path, 'r') as file:
        for line in file:
            id_part, quality_part = line.strip().split(',')
            classifications[int(id_part.split(':')[1].strip())] = quality_part.split(':')[1].strip()
    return classifications

# Function to check data consistency
def check_data_consistency(samples, frames, features):
    return all(count == samples[0] for count in samples) and all(count == 100 for count in frames) and all(count == 6 for count in features)

# Function to process each file
def process_file(file_path, min_samples, min_samples_newborn_id, samples, frames, features):
    data = np.load(file_path)
    samples.append(data.shape[0])
    frames.append(data.shape[1])
    features.append(data.shape[2])

    if data.shape[0] < min_samples:
        min_samples, min_samples_newborn_id = data.shape[0], unique_id

    if np.isnan(data).any():
        print(f"Warning: NaN values detected in file {file_path}.")
    elif np.isinf(data).any():
        print(f"Warning: Infinite values detected in file {file_path}.")
    else:
        print(f" - {file_path}: No NaN or infinite values - data is OK.")

    return min_samples, min_samples_newborn_id

# Check classification files
check_classification_files()

# Load classifications
binary_classifications = load_classifications(binary_classification_file)
three_class_classifications = load_classifications(three_class_classification_file)

# Extract unique newborn IDs
unique_ids = sorted(set(f.split(' - ')[1] for f in os.listdir(folder_path) if any(suffix in f for suffix in imu_suffixes)))

inconsistent_ids = []
healthy_count = sick_count = unknown_count = 0
min_samples = float('inf')
min_samples_newborn_id = None

# Process each newborn
for unique_id in unique_ids:
    print(f"\n===== Child ID: {unique_id} =====")
    
    # Get classifications
    id_int = int(unique_id)
    binary_quality = binary_classifications.get(id_int, 'Unknown')
    three_class_quality = three_class_classifications.get(id_int, 'Unknown')
    print(f"Binary classification: {binary_quality}\nThree-class classification: {three_class_quality}")

    # Count health classifications
    if binary_quality == 'FM+':
        healthy_count += 1
    elif binary_quality == 'FM-':
        sick_count += 1
    else:
        unknown_count += 1

    samples, frames, features = [], [], []
    print("\nFiles associated with child:")

    # Process each file
    for suffix in imu_suffixes:
        file_name = f"{unique_id} - {suffix}"
        file_path = os.path.join(folder_path, next(f for f in os.listdir(folder_path) if file_name in f))

        if os.path.exists(file_path):
            min_samples, min_samples_newborn_id = process_file(file_path, min_samples, min_samples_newborn_id, samples, frames, features)
        else:
            print(f" - {file_name}: File does not exist.")
            samples.append(None)
            frames.append(None)
            features.append(None)

    # Check consistency
    print("\nData consistency check:")
    if check_data_consistency(samples, frames, features):
        print(f"All IMU files for newborn {unique_id} have consistent dimensions: {samples[0]} samples, 100 frames, 6 features")
    else:
        inconsistent_ids.append(unique_id)

# Summary of results
print("\n===== Summary of Results =====")
if inconsistent_ids:
    print("Inconsistencies were detected in the following newborns:")
    for id in inconsistent_ids:
        print(f" - Child ID: {id}")
else:
    print("All IMU files have consistent dimensions for each newborn.")

print(f"\nSummary of health classification based on binary classification:")
print(f" - Healthy (FM+): {healthy_count}")
print(f" - Potential Issues (FM-): {sick_count}")
print(f" - Unknown: {unknown_count}")

if min_samples_newborn_id is not None:
    print(f"\nThe minimum number of samples recorded in IMU data is: {min_samples} for newborn {min_samples_newborn_id}")
else:
    print("\nNo valid IMU data found to determine the minimum number of samples.")
